"""
Simple Single-Agent Workflow with Opik Tracing

This demonstrates a basic agent workflow with:
- 1 Personal Assistant Agent
- 2 simple tools (Note Taker, Weather Checker)
- Full Opik tracing

Requirements:
- OPENROUTER_API_KEY environment variable
- Opik running locally
- python-dotenv (optional, for .env file support)

Run: python simple_agent.py
"""

# Load environment variables from .env file if python-dotenv is available
try:
    from dotenv import load_dotenv
    load_dotenv()
    ENV_FILE_LOADED = True
except ImportError:
    ENV_FILE_LOADED = False

import os
from typing import TypedDict, Annotated, Sequence
from operator import add
from datetime import datetime
import uuid

from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph
from langgraph.prebuilt import ToolNode

from opik import configure
from opik.integrations.langchain import OpikTracer
from opik.guardrails import Guardrail, PointGuardAi
from opik import exceptions

# Configure Opik for local usage
configure(use_local=True)

# Validate required environment variables
if not os.environ.get("OPENROUTER_API_KEY"):
    raise ValueError(
        "OPENROUTER_API_KEY environment variable is required.\n"
        "Please set it in your .env file or export it:\n"
        "  export OPENROUTER_API_KEY='your-api-key-here'\n"
        "Or create a .env file (see .env.example)"
    )

# Configure OpenRouter LLM
llm = ChatOpenAI(
    model="meta-llama/llama-3.1-70b-instruct",
    openai_api_key=os.environ.get("OPENROUTER_API_KEY"),
    openai_api_base="https://openrouter.ai/api/v1",
    temperature=0.7,
)

# Initialize PointGuard guardrails (optional)
guardrails_enabled = False
guard = None
policy_name = os.environ.get("POINTGUARDAI_POLICY_NAME")

if policy_name:
    try:
        guard = Guardrail(
            guards=[PointGuardAi(policy_name=policy_name)],
            guardrail_timeout=30
        )
        guardrails_enabled = True
    except Exception as e:
        print(f"âš ï¸  Warning: Failed to initialize PointGuard: {e}")
        print("   Continuing without guardrails...")

print("ğŸš€ Simple Agent Workflow Starting...")
print("ï¿½ Environment: {'.env loaded' if ENV_FILE_LOADED else 'system variables'}")
print("ğŸ“Š Model: meta-llama/llama-3.1-70b-instruct")
print("ğŸ” Opik tracing: Enabled")
print("ğŸ”‘ API Key: {'âœ“ Set' if os.environ.get('OPENROUTER_API_KEY') else 'âœ— Missing'}")
if guardrails_enabled:
    print("ğŸ›¡ï¸  PointGuard: Enabled (Policy: {policy_name})")
else:
    print("ğŸ›¡ï¸  PointGuard: Disabled")
    print("-" * 60)

# ============================================================================
# SIMPLE TOOLS
# ============================================================================

# In-memory note storage
notes = []

@tool
def save_note(content: str) -> str:
    """Save a note to memory. Use this to remember important information."""
    note_id = len(notes) + 1
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    note = {
        "id": note_id,
        "content": content,
        "timestamp": timestamp
    }
    notes.append(note)
    return f"âœ… Note #{note_id} saved at {timestamp}: {content}"

@tool
def get_weather(location: str) -> str:
    """Get weather information for a location. Returns mock weather data."""
    # Mock weather data
    weather_data = {
        "new york": "â˜€ï¸ Sunny, 72Â°F (22Â°C), Light breeze",
        "london": "ğŸŒ§ï¸ Rainy, 55Â°F (13Â°C), Moderate wind",
        "tokyo": "â›… Partly cloudy, 68Â°F (20Â°C), Calm",
        "paris": "â˜ï¸ Cloudy, 60Â°F (16Â°C), Light wind",
        "sydney": "ğŸŒ¤ï¸ Mostly sunny, 78Â°F (26Â°C), Gentle breeze"
    }
    
    location_lower = location.lower()
    for city in weather_data:
        if city in location_lower:
            return f"Weather in {location}: {weather_data[city]}"
    
    # Default response for unknown locations
    return f"Weather in {location}: â›… Partly cloudy, 65Â°F (18Â°C), Light breeze (simulated)"

# Collect tools
tools = [save_note, get_weather]
tool_node = ToolNode(tools)

# Bind tools to LLM
llm_with_tools = llm.bind_tools(tools)

# ============================================================================
# STATE DEFINITION
# ============================================================================

class AgentState(TypedDict):
    """Simple state for the agent."""
    messages: Annotated[Sequence[BaseMessage], add]
    user_input: str
    final_response: str

# ============================================================================
# AGENT NODE
# ============================================================================

def assistant_agent(state: AgentState):
    """Personal Assistant Agent: Handles user requests using available tools."""
    print("\nğŸ¤– ASSISTANT: Processing request...")
    
    user_input = state["user_input"]
    
    # Generate unique correlation key for this request
    correlation_key = "Opik-test"
    
    # Validate input with PointGuard (if enabled)
    if guardrails_enabled and guard:
        try:
            user_input = guard.validate_and_get_input(user_input, correlation_key=correlation_key)
            print("   âœ… Input validation passed")
        except exceptions.GuardrailValidationFailed as e:
            print(f"   âŒ Input blocked: {e.failed_validations}")
            return {
                "messages": [],
                "final_response": f"âš ï¸ Your request was blocked by content policy: {e.failed_validations}"
            }
    
    system_prompt = """You are a helpful Personal Assistant with access to tools.
    
    Available tools:
    - save_note: Save important information to memory
    - get_weather: Check weather for any location
    
    When the user asks about weather, use get_weather.
    When the user wants to remember something, use save_note.
    Be friendly and helpful!"""
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_input)
    ]
    
    # Invoke with tools
    response = llm_with_tools.invoke(messages)
    
    # Check if tools were called
    if response.tool_calls:
        print("   ğŸ”§ Using {len(response.tool_calls)} tool(s)...")
        tool_results = tool_node.invoke({"messages": [response]})
        final_messages = messages + [response] + tool_results["messages"]
        final_response = llm.invoke(final_messages)
        print("   âœ… Response generated")
        response_content = final_response.content
    else:
        print("   âœ… Response generated")
        response_content = response.content
    
    # Validate output with PointGuard (if enabled)
    if guardrails_enabled and guard:
        try:
            response_content = guard.validate_and_get_output(
                user_input, response_content, correlation_key=correlation_key
            )
        except exceptions.GuardrailValidationFailed as e:
            print(f"   âŒ Output blocked: {e.failed_validations}")
            return {
                "messages": [response] if not response.tool_calls else [response, final_response],
                "final_response": f"âš ï¸ The response was blocked by content policy: {e.failed_validations}"
            }
    
    return {
        "messages": [response] if not response.tool_calls else [response, final_response],
        "final_response": response_content
    }

# ============================================================================
# WORKFLOW DEFINITION
# ============================================================================

# Build simple workflow
workflow = StateGraph(AgentState)
workflow.add_node("assistant", assistant_agent)
workflow.set_entry_point("assistant")
workflow.add_edge("assistant", END)

# Compile
app = workflow.compile()

# ============================================================================
# EXECUTION
# ============================================================================

def run_query(question: str, tracer: OpikTracer):
    """Run a single query through the workflow."""
    print(f"\n{'='*60}")
    print(f"â“ Question: {question}")
    
    initial_state = {
        "messages": [],
        "user_input": question,
        "final_response": ""
    }
    
    result = app.invoke(
        initial_state,
        config={"callbacks": [tracer]}
    )
    
    print(f"\nğŸ’¬ Response: {result['final_response']}")
    print(f"{'='*60}")
    
    return result

if __name__ == "__main__":
    # Create Opik tracer
    tracer = OpikTracer(graph=app.get_graph(xray=True))
    
    print("\nğŸ¬ Running test queries...\n")
    
    # Test 1: Weather query (uses get_weather tool)
    run_query("What's the weather like in New York? 469-12-4453", tracer)
    run_query("What's the weather like in New York? abc@gmail.com", tracer)
    run_query("What's the weather like in New York?", tracer)

    # # Test 2: Note-taking (uses save_note tool)
    # run_query("Remember that I have a meeting with Sarah at 3pm tomorrow", tracer)
    
    # # Test 3: Combined query (may use both tools)
    # run_query("Check Tokyo weather and save a note that I'm planning a trip there", tracer)
    
    # # Test 4: Simple query (no tools needed)
    # run_query("Tell me a fun fact about penguins", tracer)
    
    print("\nâœ… All queries completed!")
    print("ğŸ“Š Check your Opik dashboard for trace visualization")
    print("ğŸ“ Notes saved: {len(notes)}")
    if notes:
        print("\nSaved notes:")
        for note in notes:
            print(f"  - #{note['id']}: {note['content']}")
    print()
